# 无监督学习

无监督学习（unsupervised learning），是机器学习的一种方法，它没有事先标记过的训练示例，而是直接对输入的数据进行建模，挖掘数据之间的潜在关系，自动对输入的资料进行分类或分群。无监督学习的主要运用包含：聚类算法（cluster analysis）、关联规则（association rule）、维度缩减（dimensionality reduce）。他是监督式学习和强化学习之外的一种选择。
无监督学习模型用于执行三大任务：聚类、关联和降维。下面我们将定义每种学习方法，并重点介绍有效执行这些学习方法的常用算法和途径。

## 聚类

聚类是一种数据挖掘技术，该技术根据未标记数据的相似性或差异性对其进行分组。聚类算法用于将未分类的原始数据对象处理为不同的组，通过信息的结构或模式来表示这些组。聚类算法可以分为几种类型，具体为独占、重叠、分层和概率。

### 独占和重叠聚类

独占聚类是一种分组形式，它规定一个数据点只能存在于一个聚类中。这也可以称为”硬”聚类。K均值聚类算法是独占聚类的一个例子。

K均值聚类是独占聚类方法的一个常见示例，其中数据点被分配到K个组中，K表示基于质心距离而分类的聚类数。最接近给定质心的数据点将在同一类别下聚类。K值较大指示分组较小，粒度更大，而K值较小则表明分组较大，粒度较小。K均值聚类常用于市场细分、文档聚类、影像分割和影像压缩。

K均值聚类的算法步骤如下：

1. 定义K个质心。一开始这些质心是随机的（也有一些更加有效的用于初始化质心的算法）
2. 寻找最近的质心并更新聚类分配。将每个数据点都分配给这K个聚类中的一个。每个数据点都被分配给离它最近的质心的聚类。这里的接近程度的度量是一个超参数——通常是欧氏距离（Euclidean distance）。
3. 将质心移动到它们的聚类的中心。每个聚类的质心的新位置是通过计算该聚类中所有数据点的平均位置得到的。
4. 重复第二、三步，直到每次迭代时质心的位置不再显著变化（即直到该算法收敛）。

其过程如下图所示：

![img](D:\notes\markdown\无监督学习.assets\58436-2019-04-11-k-maens.gif)

重叠聚类与独占聚类的不同之处在于，重叠聚类允许数据点属于具有不同隶属程度的多个聚类。“软”或模糊K均值聚类是重叠聚类的一个示例。

### 分层聚类

分层聚类也称为分层聚类分析（HCA），这是一种无监督聚类算法，可以分为两种方式；它们可以是聚合的，也可以是分裂的。凝聚聚类被认为时一种“自下而上的方法”。凝聚聚类的数据点起初被划入单独的分组，然后再根据相似性反复合并在一起，直到实现一个聚类。分裂聚类的定义方式刚好与凝聚聚类相反；分裂聚类采用“自上而下”的方法。在这种情况下，将根据数据点之间的差异来划分单个数据聚类。分裂聚类一般不常用。

分层聚类过程通常使用系统树图（一种树状图）直观呈现出来，该图记录了每次迭代中数据点的合并或拆分。结合下图我们讲解一下凝聚聚类的步骤：

![img](D:\notes\markdown\无监督学习.assets\c1b62e32-6e9c-4157-84a4-fdc949d84ec7.png)

1. 首先从N个聚类开始，每个数据点自成一个聚类。
2. 将彼此靠的最近的两个聚类融合为一个。现在共有N-1个聚类。
3. 重新计算这些聚类之间的距离。
4. 重复第二和第三步，直到得到包含N个数据点的一个聚类。
5. 选择一个聚类数量，然后在这个树状图中划一条水平线得到最终结果。

## 关联规则

关联规则是一种基于规则的方法，用于发现给定数据集中各变量之间的关系。这些方法常用于市场购物篮分析，使企业能够更好地了解不同产品之间的关系。了解顾客的消费习惯有助于企业制定更好的交叉销售策略，开发更出色的推荐引擎。虽然有几种不同的算法用于生成关联规则，例如 Apriori、Eclat 和 FP-Growth，但 Apriori 算法使用最为广泛。

Apriori 算法采用广度优先搜索算法进行搜索并采用树结构来对候选项目集进行高效计数。它通过长度为k-1的候选项目集来产生长度为k的候选项目集，然后从中删除包含不常见子模式的候选项。根据向下封闭性原理，该候选项目集包含所有长度为k的频繁项目集。之后，就可以通过扫描交易数据库来决定候选项目集中的频繁项目集。

Apriori算法流程如下：

1. 扫描整个数据集，得到所有出现过的数据，作为候选频繁1项集。

2. 挖掘频繁k项集

   a. 扫描数据计算候选频繁k项集的支持度

   b. 去除候选频繁k项集中支持度低于阈值的数据集，得到频繁k项集。如果得到的频繁k项集为空，则直接返回频繁k-1项集的集合作为算法结果，算法结束。如果得到的频繁k项集只有一项，则直接返回频繁k项集的集合作为算法结果，算法结束。

   c. 基于频繁k项集，连接生成候选频繁k+1项集。

3. 令k=k+1，转入·步骤2。

4. 利用频繁项集构造出满足最小置信度的规则（由频繁项集产生强关联）

支持度：支持度就是关联的数据在数据集中出现的次数占总数据集的比重。或者说数据关联出现的概率。如果我们有两个想分析关联性的数据X和Y，则对应的支持度为：$Support(X,Y)=P(XY)=\frac{number(XY)}{number(ALLSamples)}$。以此类推，如果我们有三个想分析关联性的数据X，Y和Z，则对应的支持度为：$Support(X,Y,Z)=P(XYZ)=\frac{number(XYZ)}{number(ALLSamples)}$

频繁项集：满足最小支持度的所有项集，称作频繁项集

置信度：置信度体现了一个数据出现后，另一个数据出现的概率，或者说数据的条件概率。如果我们有两个想分析关联性的数据X和Y，X对Y的置信度为：$Confidence(X, Y)=P(X|Y)=\frac{P(XY)}{P(Y)}$。以此类推，比如对于三个数据X，Y，Z，则X对于Y和Z的置信度为：$Confidence(X, Y, Z)=P(X|YZ)=\frac{P(XYZ)}{P(YZ)}$

我们可以举例来说明：一个大型超级市场根据最小存货单位（SKU）来追踪每件物品的销售数据。从而也可以得知哪里物品通常被同时购买。通过采用先验算法来从这些销售数据中创建频**繁购买商品组合**的清单是一个效率适中的方法。假设交易数据库包含以下子集{1,2,3,4}，{1,2}，{2,3,4}，{2,3}，{1,2,4}，{3,4}，{2,4}。每个标号表示一种商品，如“黄油”或“面包”。先验算法首先要分别计算单个商品的购买频率。下表解释了先验算法得出的单个商品购买频率。

| 商品编号 | 购买次数 |
| -------- | -------- |
| 1        | 3        |
| 2        | 6        |
| 3        | 4        |
| 4        | 5        |

然后我们可以定义一个最少购买次数来定义所谓的“频繁”。在这个例子中，我们定义最少的购买次数为3（支持度的阈值即为3/7）。因此，所有的购买都为频繁购买。接下来，就要生成频繁购买商品的组合及购买频率。先验算法通过修改树结构中的所有可能子集来进行这一步骤。然后我们仅重新选择频繁购买的商品组合：

| 商品编号 | 购买次数 |
| -------- | -------- |
| {1,2}    | 3        |
| {2,3}    | 3        |
| {2,4}    | 4        |
| {3,4}    | 3        |

并且生成一个包含3件商品的频繁组合列表（通过将频繁购买商品组合与频繁购买的单件商品联系起来得出）。在上述例子中，不存在包含3件商品组合的频繁组合。最常见的3件商品组合为{1,2,4}和{2,3,4}，但是他们的购买次数为2，低于我们设定的最低购买次数。

## 降维

虽然更多的数据通常会产生更准确的结果，但这也会影响机器学习算法的性能（例如过度拟合），并且还会造成数据集可视化的难度提高。 如果在给定数据集中的特征或维度数量过多，便会用到降维这种技术。 降维可将输入的数据量减少到可管理的大小，同时尽可能地保持数据集的完整性。 在数据预处理阶段通常会使用降维技术，可以采用几种不同的降维方法，例如：

### 主成分分析

主成分分析（简称 PCA）是一种降维算法，用于减少冗余并通过特征提取来压缩数据集。 这种方法使用线性变换来创建新的数据表示，从而产生一组“主成分”。

参考PCA降维动画演示：https://www.bilibili.com/video/BV1QS4y1e7y6/?share_source=copy_web&vd_source=fd14c7c40291d5282ac89695b8274498

主成分分析经常用减少数据集的维数，同时保持数据集的对方差贡献最大的特征。这是通过保留低阶主成分，忽略高阶主成分做到的。这样低阶成分往往能够保留住数据的最重要方面。

其基本思想：

* 将坐标轴中心移到数据的中心，然后旋转坐标轴，使得数据在C1轴上的方差最大，即全部n个数据个体在该方向上的投影最为分散，意味着更多的信息被保留下来。C1成为**第一主成分**。
* C2**第二主成分**：找一个C2，使得C2与C1的协方差为0（C2与C1正交），以免与C1信息重叠，并且使数据在该方向的方差尽量最大。
* 以此类推，找到第三主成分，第四主成分……第p个主成分。p个随机变量可以有p个主成分。

变换的步骤如下：

1. 计算矩阵X的样本的协方差矩阵S（此为不标准PCA，标准PCA计算相关系数矩阵C）
2. 计算协方差矩阵S（或C）的最大的d个特征向量e1, e2, …, ed和对应特征值
3. 投影数据到特征向量张成的空间之中。利用下面公式，其中BV值是原样本中对应维度的值。![img](D:\notes\markdown\无监督学习.assets\49679b0b-39c6-4135-bc39-f1d50f7bca22.png)

PCA降维需要找到样本协方差矩阵$X^TX$的最大的 d 个特征向量，然后用这最大的d个特征向量张成的矩阵来做低维投影降维。这里d个特征向量张成的矩阵即通过SVD奇异值分解得到右奇异矩阵V，新得到的矩阵$X'_{m*k}=X_{m*n}V_{n*k}$

### 奇异值分解

奇异值分解（简称 SVD）是另一种降维方法，它将矩阵M分解为三个低秩矩阵。SVD 由公式 $M=UΣV^T$ 表示，其中M是一个m x n的矩阵，U是一个m x m的正交矩阵，V是一个n x n的正交矩阵，Σ是m x n的对角矩阵。

下图就是SVD的定义：

![img](D:\notes\markdown\无监督学习.assets\78a8df33-5d89-43a7-8444-139d664a6cc8.png)

U 的列向量就是$MM^T$的特征向量，U 中的每个特征向量叫做M的**左奇异向量**；V 的列向量就是$M^TM$的特征向量，V中的每个特征向量叫做M的**右奇异向量**。Σ的对角线元素（非零奇异值）就是$MM^T$或$M^TM$的非零特征值的均方根，奇异值按由大到小分布在主对角线上，其余值为0，一般奇异值数量远小于m和n，故有很多补零。

根据矩阵的奇异值分解就可以对原始矩阵进行数据压缩了，对于一个 m × n 的矩阵M，假设其奇异值个数为r，r通常是远小于m和n的，在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上的比例。于是，我们也可以用最大的k个的奇异值和对应的左右奇异向量来近似描述矩阵。如下图：

![img](D:\notes\markdown\无监督学习.assets\3463eeb3-c2be-49ed-bebb-a0b9af530c7a.png)

通过保留较大的奇异值，可以选择保留矩阵中最重要的结构，从而实现数据的降维和压缩，这在数据分析和图像处理中非常有用。

在推荐系统中，我们首先对于评分矩阵$M_{m*n}$中的缺失值进行简单补全，比如用全局平均值，得到补全后的矩阵$M'$，接着通过SVD并取最大的k个奇异值将$M'$分解成$M'=U_{m*k}Σ_{k*k}V^T_{k*n}$，从而得到一个降维后的评分矩阵，其中，$M'(u, i) = U^T_iΣ_{k*k}V_j$就是第i个用户对第j个物品的评分预测。



# 自监督学习

自监督学习（self-supervised learning）， 一般是在神经网络中使用的一种无监督方法。在自监督学习中，系统学习从其输入的一部分预测其输入的另外一部分，换句话说，一部分的输入作为监督信号和输入的剩余部分一起送入到系统。有监督学习和自监督学习的区别如下：

![img](D:\notes\markdown\无监督学习.assets\567e27cf-f561-48c7-9c46-a48265babca8.png)

## BERT

知名的自监督学习网络包括BERT和GPT。BERT的核心部分是一个Transformer模型，最初的英语BERT发布时提供两种类型的预训练模型：（1）BERT-BASE模型，一个12层，768维（token映射到的低维连续向量空间的维度，可使用word2vector等框架进行词嵌入），12个自注意头（self attention head），110M参数的神经网络结构；（2）BERT-LARGE模型，一个24层，1024维，16个自注意头，340M参数的神经网络结构。两者的训练语料都是BooksCorpus以及英语维基百科语料，单词量分别是8亿以及25亿。

BERT在两个任务上进行预训练：语言模型（15%的token被掩盖，BERT需要从上下文中进行推断）和下一句预测（BERT需要预测给定的第二个句子是否是第一句的下一句）。训练完成后，BERT学习到单词的上下文嵌入。

BERT训练如下图：

![img](D:\notes\markdown\无监督学习.assets\1bc22b80-7d9b-4e9e-8adc-ff80b34611eb.png)

![img](D:\notes\markdown\无监督学习.assets\7359aae0-e5e1-4389-8687-86175fb0d63f.png)

text、tokens、ids形如：

~~~
text: "5月10日，中国举重队 [MASK] 布了参加巴黎奥 [MASK] 会的阵容 [MASK] 单。"
tokens: ['5', '月', '10', '日', '，', '中', '国', '举', '重', '队', '[MASK]', '布', '了', '参', '加', '巴', '黎', '奥', '[MASK]', '会', '的', '阵', '容', '[MASK]', '单', '。']
ids: [126, 3299, 8108, 3189, 8024, 704, 1744, 715, 7028, 7339, 103, 2357, 749, 1346, 1217, 2349, 7944, 1952, 103, 833, 4638, 7347, 2159, 103, 1296, 511]
~~~

针对上面的文本，其第一个遮盖文字使用BERT进行预测结果如下：

~~~
output: [-12.2944, -11.4608, -12.4523,  ..., -11.8836,  -7.4780,  -8.9830] (len:21128)
Top 3 probabilities: [0.7737, 0.1811, 0.0396]
ids: [1062, 2146, 1355]
tokens: ['公', '宣', '发']
~~~

测试代码如下：

~~~python
# BERT理解上下文的语言代表模型
# BERT结构为Transformer中的Encoder

import torch
from IPython.display import clear_output
from transformers import AutoTokenizer, AutoModelForMaskedLM

tokenizer = AutoTokenizer.from_pretrained("bert-base-chinese")
model = AutoModelForMaskedLM.from_pretrained("bert-base-chinese")

vocab = tokenizer.vocab
print("字典大小：", len(vocab))

# 10 18 23
text = "5月10日，中国举重队 [MASK] 布了参加巴黎奥 [MASK] 会的阵容 [MASK] 单。"
tokens = tokenizer.tokenize(text)
ids = tokenizer.convert_tokens_to_ids(tokens)

tokens_tensor = torch.tensor([ids])
clear_output()

model.eval()
with torch.no_grad():
    outputs = model(tokens_tensor)
    predictions = outputs[0]
    print(predictions[0][10])
    print(predictions.shape)
del model

# 将 [MASK] 位置的概率分布取 top 3 最有可能的 tokens 出来
probs_10, indices_10 = torch.topk(torch.softmax(predictions[0, 10], -1), 3)
predicted_tokens_10 = tokenizer.convert_ids_to_tokens(indices_10.tolist())
probs_18, indices_18 = torch.topk(torch.softmax(predictions[0, 18], -1), 3)
predicted_tokens_18 = tokenizer.convert_ids_to_tokens(indices_18.tolist())
probs_23, indices_23 = torch.topk(torch.softmax(predictions[0, 23], -1), 3)
predicted_tokens_23 = tokenizer.convert_ids_to_tokens(indices_23.tolist())


# 显示 top k 可能的字。一般我們就是取 top 1 当作预测值
for i, (t_10, p_10, t_18, p_18, t_23, p_23) in enumerate(zip(predicted_tokens_10, probs_10, predicted_tokens_18, probs_18, predicted_tokens_23, probs_23), 1):
    tokens[10] = t_10
    tokens[18] = t_18
    tokens[23] = t_23
    print("Top {} ({:2}% {:2}% {:2}%)：{}".format(i, int(p_10.item() * 100), int(p_18.item() * 100), int(p_23.item() * 100), tokens))
~~~

## GPT-2

GPT-2其前身是GPT，其全名为Generative Pre-Training，它本体实际上就是Transformer里的Decoder，GPT-2有三种不同参数量的模型：GPT-2 Small（124M）、GPT-2 Large（774M）、GPT-2 Medium（355M）。

对于GPT-2，其输入是一定长度的连续文本序列，目标是向左移动一个token（单词或单词片段）的相同序列。该模型在内部使用mask-mechanism来保证token i输出仅使用来自1到i的输入，而不使用将来的token。

GPT框架及训练如下图：

![img](D:\notes\markdown\无监督学习.assets\a0264da6-b64b-40e1-b87b-77fb30db8669.png)

GPT-2模型中Masked Multi-Head Attention实现代码如下：

~~~python
def attention_mask(nd, ns, *, dtype):
    # 1's in the lower triangle, counting from the lower right corner.
    # 注意力掩码用于防止模型关注未来的位置
    i = tf.range(nd)[:,None]	
    j = tf.range(ns)
    m = i >= j - ns + nd
    return tf.cast(m, dtype)

def mask_attn_weights(w):
    # w has shape [batch, heads, dst_sequence, src_sequence], where information flows from src to dst.
    # 应用注意力掩码到权重张量中，在掩码为1的位置上保留原权重，在掩码为0的位置上将权重设置为一个非常大的负数
    _, _, nd, ns = shape_list(w)				# 获取权重张量的形状
    b = attention_mask(nd, ns, dtype=w.dtype)	# 计算注意力掩码
    b = tf.reshape(b, [1, 1, nd, ns])			# 将注意力掩码的形状调整为 [1, 1, nd, ns]
    w = w*b - tf.cast(1e10, w.dtype)*(1-b)		# 应用注意力掩码并修改权重
    return w

def multihead_attn(q, k, v):
    # q, k, v have shape [batch, heads, sequence, features]
    w = tf.matmul(q, k, transpose_b=True)	# 计算q(查询, query)和k(键，key)的点积
    w = w * tf.rsqrt(tf.cast(v.shape[-1].value, w.dtype)	# 除以特征维度的平方根来缩放权重
    w = mask_attn_weights(w)	# 应用注意力掩码
    w = softmax(w)				# 计算注意力权重，掩码为0的位置上的权重值接近于0
    a = tf.matmul(w, v)			# 计算最终的输出
    return a
~~~

GPT-2测试：

~~~python
from transformers import pipeline, set_seed
generator = pipeline('text-generation', model='gpt2')
set_seed(42)
result = generator("I have a dream that", max_length=15, num_return_sequences=5)
print(result)

'''
[{'generated_text': 'I have a dream that I can do something. I want to do something'}, {'generated_text': "I have a dream that there's a new, better form of society."}, {'generated_text': "I have a dream that someday I'll be a writer, editor, photographer"}, {'generated_text': 'I have a dream that a baby will be born in a year or two'}, {'generated_text': 'I have a dream that is possible: my father is not a writer and'}]
'''
~~~



## 参考

Transformer是一种seq2seq model，Transformer由Encoder和Decoder两部分组成，它们都用到了self-attention机制。BERT属于Transformer模型，它使用了自己设计的Encoder，GPT也属于Transformer模型，它设计自己的Decoder。

Transformer模型架构如下图，其中左边部分是Encoder，右边部分是Decoder。

<img src="D:\notes\markdown\无监督学习.assets\5ed5a521-cee7-4e76-b7d7-c6bd5c58d5d3.png" alt="img" style="zoom: 33%;" />

self-attention机制图示图下，其中$W^q$、$W^k$、$W^v$

<img src="D:\notes\markdown\无监督学习.assets\eea8580b-2f65-499e-87ec-7ce1ef73e26d.png" alt="img" style="zoom: 67%;" />

<img src="D:\notes\markdown\无监督学习.assets\cac0b839-bf84-4bed-823c-746cf2b61c72.png" alt="img" style="zoom:67%;" />

<img src="D:\notes\markdown\无监督学习.assets\e07c97b3-c3f6-4df5-9116-5356c0c2f8bb.png" alt="img" style="zoom:67%;" />

BERT网络结构如下图：

<img src="D:\notes\markdown\无监督学习.assets\f44b5ae6-4f5c-4731-8530-3ea69f9fab98.png" alt="img" style="zoom:67%;" />

![img](D:\notes\markdown\无监督学习.assets\65fff3ed-15ce-4cd7-8e72-e6b0d73597f4.png)

GPT框架如下：

![img](D:\notes\markdown\无监督学习.assets\4e902d7c-9518-4420-9171-43c09b308007.png)

![img](D:\notes\markdown\无监督学习.assets\71c6dafb-a23a-4722-ba2f-0941d27297ea.png)

GPT结构如下：

![img](D:\notes\markdown\无监督学习.assets\a0264da6-b64b-40e1-b87b-77fb30db8669.png)

![img](D:\notes\markdown\无监督学习.assets\9587604f-f94b-4fa3-a95d-c3a68b4ed5bb.png)

![img](D:\notes\markdown\无监督学习.assets\ca1dd8c9-e8e7-4c3e-9f5a-da9f1b9afa80.png)

Transformer中的Cross attention：

![img](D:\notes\markdown\无监督学习.assets\5bd90477-8dbd-405d-b38f-53b76f84c9cd.png) 



MoCo: 视觉领域的无监督学习

https://www.youtube.com/watch?v=pXvMXfPJZ2M

分类效果：

![img](D:\notes\markdown\无监督学习.assets\d4feeb7d-04b8-4dd7-b8f8-19bd92fe0edb.png)

|           | dataset     | pre-train time | MoCo v1 top-1 acc. | MoCo v2 top-1 acc. | MoCo v2 top-1 acc.(epoch=800) |
| --------- | ----------- | -------------- | ------------------ | ------------------ | ----------------------------- |
| ResNet-50 | ImageNet-1M | 53 hours       | 60.8±0.2           | 67.5±0.1           | 71.1±0.1                      |

检测效果：

![img](D:\notes\markdown\无监督学习.assets\867f6656-5c6b-48d5-b406-5cec5ad851c5.png)

| pretrain                    | AP50 | AP   | AP75 |
| --------------------------- | ---- | ---- | ---- |
| ImageNet-1M, supervised     | 81.3 | 53.5 | 58.8 |
| ImageNet-1M, MoCo v1, 200ep | 81.5 | 55.9 | 62.6 |
| ImageNet-1M, MoCo v2, 200ep | 82.4 | 57.0 | 63.6 |
| ImageNet-1M, MoCo v2, 800ep | 82.5 | 57.4 | 64.0 |

分类自测：

|           | dataset          | pre-train time | MoCo v1 top-1 acc. | MoCo v2 top-1 acc. | MoCo v2 top-1 acc.(epoch=800) |
| --------- | ---------------- | -------------- | ------------------ | ------------------ | ----------------------------- |
| ResNet-50 | ImageNetMini-60k | -              | 45                 | 55                 | 62                            |

检测自测：

![img](D:\notes\markdown\无监督学习.assets\7b508aaf-aa1b-416c-b482-01437acf6722.png)



用于预训练的任务被称为代理任务

fine-tuning(针对下游任务的微调)

BERT的下游任务训练：

![img](D:\notes\markdown\无监督学习.assets\16073aac-99dc-4819-8c22-99911a3b5871.png)

![img](D:\notes\markdown\无监督学习.assets\b056600f-3245-4686-a2dc-115253b5427a.png)

mobilenet-v2：深度可分离卷积

假设输入图片维度为（ｄｉｎ，ｄｉｎ，Ｃ），其中Ｃ为图片的通道数，（ｄｉｎ，ｄｉｎ）为图片的大小，假设卷积核大小为（ｄｋ，ｄｋ，Ｃ），共有 Ｎ个，执行普通卷积计算后输出 大小为（ｄｏｕｔ，ｄｏｕｔ，Ｎ）。普通卷积总计算量Ｓ１为Ｓ１ ＝Ｎ×ｄ２ ｏｕｔ×ｄ２ ｋ ×Ｃ

深度可分离卷积计算过程分为２步。第１步为深度卷积，输入图片仍为（ｄｉｎ，ｄｉｎ， Ｃ），在深度卷积操作中卷积一次应用于单个通道， 因此卷积核大小为（ｄｋ，ｄｋ，１），共有Ｃ个，则输出 图片大小为（ｄｏｕｔ，ｄｏｕｔ，Ｃ）；第 ２步为逐点卷积，输 入图片为深度卷积的输出（ｄｏｕｔ，ｄｏｕｔ，Ｃ），在逐点卷 积操作中卷积一次应用于单个通道，因此卷积核 大小为（１，１，Ｃ），共有 Ｎ个，则输出图片大小为 （ｄｏｕｔ，ｄｏｕｔ，N）。深 度 可 分 离 卷 积 的 总 计 算 量Ｓ２为Ｓ２ ＝Ｃ×ｄ２ ｏｕｔ×（ｄ２ ｋ ＋Ｎ）

