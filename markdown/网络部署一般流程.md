# 从训练框架到NPU硬件上部署的流程
网络部署一般要经过从模型结构优化，训练框架导出，模型优化，模型量化，以及在硬件上部署的步骤。我们下面详细介绍这几个步骤。

## 模型导出

```python
import torch
import onnx
import onnxsim
import models.dense_sim as densenet

# model = torch.load('./expr/best_model_NPU')
model = densenet.Dense()
torch_model_path = './expr/best_model_NPU.pth'
model.load_state_dict(torch.load(torch_model_path, map_location='cpu'))

model.eval()

dummy_input = torch.randn(1, 3, 32, 192)
file_path = "./expr/best_model_NPU.onnx"
torch.onnx.export(model,      # model being run 
     dummy_input,             # model input (or a tuple for multiple inputs) 
     file_path,               # where to save the model
     verbose=False,
     input_names = ['input'], # the model's input names 
     output_names = ['output'],
     opset_version = 13)

model_onnx = onnx.load(file_path)
model_onnx, check = onnxsim.simplify(model_onnx)
assert check, "Simplified ONNX model could not be validated"
# onnx.save(model_onnx, file_path)
onnx.save(onnx.shape_inference.infer_shapes(model_onnx), file_path)

print('Model has been converted to ONNX')

# if __name__ == "__main__": 
#     convert()
```



## 模型结构优化（可选）

这一步主要是在不怎么影响精度情况下调整压缩模型，调整模型适合在嵌入式环境中部署。压缩网络减少网络参数量，调整模型适合硬件加速（NPU支持），如果有更多训练经验也可以做一些模型蒸馏和剪枝。

- 在全连接层前用全局平均池化代替，可以大量减少计算量以及全连接的参数。
- 尽量不使用DWCONV，分组卷积是为了减少参数量，但是我们的硬件支持多输入通道的加速，所以如果有1\*M\*k\*k的depth wise加上M\*N\*1\*1的point wise的组合，可以还原成M\*N\*k\*k的卷积。
- 尽量不使用dilated conv，空洞卷积不能用硬件加速，所以建议使用多个串联的卷积代替。
- 将两个并联的非对称卷积核还原成一个正常卷积核。为了减少参数量，会有这种非对称卷积并联替换正常卷积，例如将一个7x7的卷积拆分成了一个1x7和一个7x1。由于硬件限制，建议还原成7x7的卷积。
- 更多硬件上的限制参考硬件手册。
  

以下是分解卷积和全连接减少模型参数量的方法，但是带来的多次访存开销需要进一步在硬件上测试是否能加速。

- 分解卷积：使用两个串联小卷积核来代替一个大卷积核。两个3x3的卷积核代替一个5x5的卷积核。在效果相同的情况下，参数量仅为原先的 3x3x2 / 5x5 = 18/25。我们支持的卷积核为(1，3，5，7)。
- 分解全连接：将M\*N的全连接矩阵分解为M\*K + K\*N，只要让K<<M 且 K << N，就可以大大降低模型体积。

## 训练框架导出
导出指的是从一般的训练框架模型格式导出为ONNX模型。我们支持的网络格式解析包括ONNX和TFLite，由于使用到ONNX模型的量化工具以及ONNX作为较流行的中间模型，所以这里主要导出为ONNX模型。

ONNX对于一般的训练框架（PyTorch, TensorFlow）都有较好的支持，可以直接调用相关的API进行导出，导出后ONNX测试结果是否与之前训练框架的推理一致。一般要注意以下几点：

- 网络中一些后处理可能会在网络中进行（如yolo网络的detect），卷积之后的后处理建议都不要导出。一些后处理操作也许很简单，但可能会影响网络量化以及网络的parse，并且一般不能用硬件加速。这个操作也可以用onnx-modifier-master工具去除不想要的算子。
- 可以通过netron查看网络格式查看网络的导出结果。
- 导出的时候要设置opset>12(方便parse和量化)，设置输入形状。
- 可以使用modelcheck来测试模型的正确性。
- ONNX网络的测试一般直接比较推理结果就行，需要和训练环境的预处理后处理一致。如果是要对比每一层的结果要保证输入一致（cv和pillow处理图片结果可能不一样），数据格式一致（GPU的半精度）。
- 导出的时候可以顺便加上onnx simplify，会进行模型优化。

## 模型优化
模型优化主要是对部署的网络进行一些图优化，常量优化，算子融合等。模型优化主要发生在onnx simplify（调用onnx optimizer），量化和armnn部署上，onnx optimizer有很多优化，常见的包括：

- 常量折叠
- conv+bn的融合
- conv+addbias的融合
- matmul+addbias转换成gemm
- 融合pad到pool和conv

尽量选择onnx simplify处理模型，在加速推理的同时也防止部署的时候有不支持的parse。
还有模型优化发生在量化的时候去除clip以及在硬件中将conv+act+maxpool融合加速。

## 模型量化
量化是为了减少模型参数以及在硬件上加速，目前的量化方案包括量化感知训练QAT和训练后量化PTQ。这两种方案得到的都是伪量化的QDQ格式（量化每一个tensor）。得到的量化模型应该使用之前的训练框架导出提到的onnx推理来进行验证。需要注意的点是：

- 伪量化模型和onnx模型推理的前处理和后处理一致。但结果会有量化的误差。
- 伪量化模型在onnx runtime推理的时候会还原成浮点进行计算，和armnn的CpuRef后端基本一致，两者推理结果误差很小（舍入的误差）。量化前的模型在onnx runtime和armnn的CpuRef后端有一些区别（不确定原因，可能和GPU用float16相关），但不影响最后的推理结果。
- onnx runtime可以把每个算子的output加入到网络的output中查看每一层的输出结果。
- PTQ量化前要onnx simplif以及infer shape。
- PTQ用到的校准数据集尽量和推理环境接近

## QEMU上部署（可选）
在qemu上部署主要是为了验证自己的代码前后处理流程基本正确，以及硬件推理结果基本正确。一般部署包括以下流程：

- 确认前后处理的正确：
  
    可以先用量化前的ONNX网络推理来验证前后处理的正确，更一步准确的方法是dump推理框架的结果或者onnx推理的前后处理结果来进行比较。

- 确认浮点网络CpuRef推理正确：

    确保了前后处理正确后，如果量化前的ONNX网络在ONNX runtime上推理是正确的，但在armnn的CpuRef后端推理还是有问题，考虑parse的问题。可以dump每一层结果来定位，armnn的CpuRef的每一层结果应该很接近ONNX runtime的结果。

- 确认定点网络CpuRef推理正确：
  
    如果以上结果都正确，并且量化后的ONNX网络在ONNX runtime上推理是正确的，但量化后的ONNX网络在CpuRef后端得不到正确的结果，考虑是量化的信息没有被正确parse，（和上一步一样，可能是opset太低或太高）。

- 确认定点网络CpuAcc推理正确：

    可能是CpuAcc后端的误差或者是bug。

- 确认定点网络AlRef+CpuRef推理正确：
  
    这个后端会仿真硬件上的结果，如果结果不对，可能是量化的scale硬件上误差太大。

- 确认定点网络Alnpu+CpuAcc推理正确：

    这个后端会仿真硬件上的结果并且一些数据会从配置中读，如果结果不对，可能是生成配置的问题。配置可以设置AL_DEBUG环境变量导出相关配置信息以及硬件要的格式的配置和输入。

以上的推理过程可以dump每一层输入和输出和前一步正确的推理来对比，看是哪一层推理不对来进行进一步debug。

## 硬件上部署
QEMU上部署的步骤可以在硬件上重新测一遍，出现错误的情况也应该和在QEMU上一致，如果最后在量化后的ONNX网络Alnpu推理结果不正确，可能是配置的原因（错误的配置或者是硬件上还有一些限制）或者是硬件上的原因。